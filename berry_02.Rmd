---
title: "Bayesian Adaptive Methods for Clinical Trials - Example 2"
subtitle: "Indifference Zones"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
date: "`r format(Sys.time(), '%d %B, %Y')`"
---

The notion of an indifference zone $[\delta_l, \delta_u]$ naturally arises when comparing two treatments and a treatment effect estimate $\Delta$ that equates to the difference between the control and treatment arms. If $Pr(\Delta > \delta_u)$ is small we would favour the control and similarly if $Pr(\Delta < \delta_l)$ is small we would favour the treatment. 

We might also look at where the credible interval for $\Delta$ sits with regards to the $[\delta_l, \delta_u]$ indifference zone. If the credible interval sits entirely within the indifference zone then we can assume equivalence. However, if the credible interval crossed $\delta_l$ or was entirely below $\delta_l$ then we conclude the control is better. We can come to an analogous conclusion for the treatment if the credible interval crossed or was entirely above $\delta_u$.

TODO come back to type 1 error...

Let's consider an experiment on two groups requiring participants to complete a complex task. One group can reference an existing manual, the other has a new manual. Historically around 60% of people using it can complete the task. We want whether the new manual improves the participants ability to complete the task. A natural model for this situation is logistic regression:

$$
logit(p_i) = log \left( \frac{p_i}{1-p_i} \right) = \lambda_0 + \lambda_1 x_i
$$

The p_i corresponds to the probability of completing the task conditional on treatment group, indicated by $x_i$. The $\lambda_0$ and $\lambda_1$ are the logarithm of the baseline odds and the the treatment effect.


```{r}

# TODO
# http://www.flutterbys.com.au/stats/tut/tut12.10.html
# http://faculty.washington.edu/jonno/SISG-2011/lectures/JW-Lec3_2.pdf
# http://faculty.washington.edu/jonno/SISMIDmaterial/2-RINLA.pdf
# https://www.precision-analytics.ca/blog-1/inla
# https://www.ntnu.edu/employees/thiago.guerrera



# Need INLA:
# install.packages("INLA", repos=c(getOption("repos"), INLA="https://inla.r-inla-download.org/R/stable"), dep=TRUE)

suppressMessages(library("rethinking"))
suppressMessages(library("tidyverse"))
suppressMessages(library("INLA"))
suppressMessages(library("pwr"))

# Suggests we need ~350 per arm to see such a difference
pwr.2p.test(h = ES.h(p1 = 0.6, p2 = 0.7), sig.level = 0.05, power = .80)

p_ctl <- 0.6
p_trt <- 0.7

lambda_0 <- log(p_ctl / (1-p_ctl))
lambda_1 <- log(p_trt / (1-p_trt)) - lambda_0 

n_p_arm <- 400

x <- rep(0:1, each = n_p_arm)
set.seed(4)
linpred <- lambda_0 + lambda_1 * x
y.pred <- rethinking::inv_logit(linpred)
y <- rbinom(length(x), size = 1, prob = y.pred)
table(x, y)

df_mod <- data.frame(x = x, y = y)

summary(lm1 <- glm(y ~ x, family = "binomial", data = df_mod))

lm2 <- inla(y ~ x, 
            family = "binomial", 
            data = df_mod, 
            Ntrials=rep(1, nrow(df_mod)),
            control.compute=list(config = TRUE))
summary(lm2)

# To get the credible interval for the parameter estimate
lm2$summary.fixed

# This is just the density at each value of x. 
# Therefore it is the literal pdf rather than samples from the post.
# Note that the untransformed version integrates to 1 but as
# soon as you exponentiate to get the odds ratio, you are dealing with
# an improper prior.
post <- data.frame(lm2$marginals.fixed[[2]])
head(post)

# post a true pdf, i.e integrates to 1:
DescTools::AUC(post$x, post$y)
DescTools::AUC(exp(post$x), post$y)

# post$x <- exp(post$x)
# 
# post <- post[post$x > 0.4 & post$x < 3.1,]
# 
# integrate(approxfun(post$x, post$y), lower = 0.55, upper = 2.9)





# Max aposterior
map_1 <- rethinking::map(
  alist(
    y ~ dbinom(1, p),
    logit(p) <- b0 +  b1 * x,
    b0 ~ dnorm(0, 10),
    b1 ~ dnorm(0, 10)
  ),
  data = df_mod
)

precis(map_1)
post_map <- extract.samples(map_1)

plot(density(post_map$b1))


dens <- density(exp(post_map$b1))
DescTools::AUC(dens$x, dens$y)
df_post_map <- data.frame(x = dens$x, y = dens$y)

# Compare
exp(confint(lm1))
quantile(exp(post[,2]), probs = c(0.025, 0.975))
quantile(exp(post_map[,2]), probs = c(0.025, 0.975))

ggplot(post, aes(x=exp(x), y=y)) +
  geom_line()+
  scale_x_continuous(limits = c(0, 10)) +
  geom_vline(xintercept = exp(lambda_1)) +
  geom_line(data = df_post_map, aes(x = x, y = y), 
               inherit.aes = F, colour = "red")


```

The preliminary study looks promising, should we enrol more participants? Let's simulate and see if keeping the trial going makes sense. First we simulate the number of events arising from 1000 separate trial populations each comprising 200 patients.

```{r}
n2 <- 200

# Sample from post_1 to obtain the distribution of successful events.
n_trial_pops <- 1000
n_sims <- 1e4
x2 <- rbinom(n_trial_pops, n2, prob = post_1)

# This is the distribution of events across all of the simulated populations.
rethinking::simplehist(x2)
```

So typically, we get about 185 to 190 successes out of 200. We iterate through each of the simulated trial populations and generate the posterior distribution based on the number of successful events at the end of each study.

```{r}
post_2 <- numeric(n_sims * n_trial_pops)
dim(post_2) <- c(n_sims, n_trial_pops)

for(i in 1:length(x2)){
  post_2[,i] <- rbeta(n_sims, 
                shape1 = x1 + x2[i] + 1, 
                shape2 = n1 + n2 - x1 - x2[i] + 1)
}

```

We can examine the posteriors arising from a subset of our simualted populations to get a sense of the heterogenity.

```{r}
df_post_2 <- as.data.frame(post_2) 

idx <- sample(1:1000, size = 10, replace = F)
df_post_2_sub <- df_post_2[,idx]
df_post_2_sub <- tidyr::gather(df_post_2_sub, "sim", "p")

ggplot(df_post_2_sub, aes(x = p, group = sim)) +
  geom_density()

```

Each trial produces an empirical distribution for $p^*_j$ and there is quite a bit of variation as seen above. However, our particular interest is in a pre-specified quantile ($p_{0.025}$) from each of these posteriors. We extract the lower bounds from each of our posterior distributions and then plot the distribution of this quantile.

```{r}
post_2_q_025 <- apply(post_2, 2, quantile, 0.025)
rethinking::dens(post_2_q_025)
```

Finally, we assess the probability of this quantile (that represents a lower bound from each of the simulated studies) against our decision criteria. Our conclusion is that, on the basis of the simulations and the data collected to date, there is a good chance that the trial will have a success rate in excess of our threshold criteria and so we should go ahead and enrol the next $n_2$ patients.

```{r}
# Descision threshold. 
# We compute Pr(p_0.025 > 0.85) = number of p^*_0.025 > 0.85 / N_rep

prob_p_025 <- sum(post_2_q_025 > 0.85) / length(post_2_q_025)

sprintf("Probability that 2.5%% quantile > 0.8 = %.3f ", prob_p_025)

ifelse(prob_p_025 > 0.8, 
       paste("Continue trial"), 
       paste("Discontinue trial")) 
```







