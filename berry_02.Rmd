---
title: "Bayesian Adaptive Methods for Clinical Trials - Example 2"
subtitle: "Indifference Zones"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
date: "`r format(Sys.time(), '%d %B, %Y')`"
---

The notion of an indifference zone $[\delta_l, \delta_u]$ naturally arises when comparing two treatments and a treatment effect estimate $\Delta$ equating to the difference between the control and treatment arms. We may abandon our experiment if $Pr(\Delta > \delta_u)$ is sufficiently small (thus favouring the control) and similarly if $Pr(\Delta < \delta_l)$ is sufficiently small (favouring the treatment). 

 

```{r}
library(rethinking)
library(tidyverse)


p_ctl <- 0.2
p_trt <- 0.1

lambda_0 <- log(p_ctl / (1-p_ctl))
lambda_1 <- log(p_trt / (1-p_trt)) - lambda_0 

or_trt <- exp(lambda_1)






```

The preliminary study looks promising, should we enrol more participants? Let's simulate and see if keeping the trial going makes sense. First we simulate the number of events arising from 1000 separate trial populations each comprising 200 patients.

```{r}
n2 <- 200

# Sample from post_1 to obtain the distribution of successful events.
n_trial_pops <- 1000
n_sims <- 1e4
x2 <- rbinom(n_trial_pops, n2, prob = post_1)

# This is the distribution of events across all of the simulated populations.
rethinking::simplehist(x2)
```

So typically, we get about 185 to 190 successes out of 200. We iterate through each of the simulated trial populations and generate the posterior distribution based on the number of successful events at the end of each study.

```{r}
post_2 <- numeric(n_sims * n_trial_pops)
dim(post_2) <- c(n_sims, n_trial_pops)

for(i in 1:length(x2)){
  post_2[,i] <- rbeta(n_sims, 
                shape1 = x1 + x2[i] + 1, 
                shape2 = n1 + n2 - x1 - x2[i] + 1)
}

```

We can examine the posteriors arising from a subset of our simualted populations to get a sense of the heterogenity.

```{r}
df_post_2 <- as.data.frame(post_2) 

idx <- sample(1:1000, size = 10, replace = F)
df_post_2_sub <- df_post_2[,idx]
df_post_2_sub <- tidyr::gather(df_post_2_sub, "sim", "p")

ggplot(df_post_2_sub, aes(x = p, group = sim)) +
  geom_density()

```

Each trial produces an empirical distribution for $p^*_j$ and there is quite a bit of variation as seen above. However, our particular interest is in a pre-specified quantile ($p_{0.025}$) from each of these posteriors. We extract the lower bounds from each of our posterior distributions and then plot the distribution of this quantile.

```{r}
post_2_q_025 <- apply(post_2, 2, quantile, 0.025)
rethinking::dens(post_2_q_025)
```

Finally, we assess the probability of this quantile (that represents a lower bound from each of the simulated studies) against our decision criteria. Our conclusion is that, on the basis of the simulations and the data collected to date, there is a good chance that the trial will have a success rate in excess of our threshold criteria and so we should go ahead and enrol the next $n_2$ patients.

```{r}
# Descision threshold. 
# We compute Pr(p_0.025 > 0.85) = number of p^*_0.025 > 0.85 / N_rep

prob_p_025 <- sum(post_2_q_025 > 0.85) / length(post_2_q_025)

sprintf("Probability that 2.5%% quantile > 0.8 = %.3f ", prob_p_025)

ifelse(prob_p_025 > 0.8, 
       paste("Continue trial"), 
       paste("Discontinue trial")) 
```







